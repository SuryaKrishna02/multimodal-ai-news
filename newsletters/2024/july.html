<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MultiModal AI Newsletter - July 2024 Edition</title>
    <link rel="icon" type="image/png" href="../../src/assets/multimodal.jpg">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../src/styles/newsletter.css">
</head>

<body>
    <header>
        <nav>
            <a href="../../index.html">Home</a>
            <a href="https://sites.google.com/cohere.com/c4ai-community/community-programs/multimodal"
                target="_blank">Community</a>
            <a href="../../index.html#about-us" target="_blank">About Us</a>
        </nav>
    </header>

    <main>
        <section class="ai-zone floating">
            <h1>Multimodal AI Newsletter</h1>
            <h2>July 2024 Edition</h2>
        </section>

        <section class="tech-pulse">
            <h2>The Tech Pulse</h2>
            <div class="item">
                <h3>Gen3 Alpha from Runway: New Video Generation Model</h3>
                <p><i>Gen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure
                        built for large-scale multimodal training. It is a major improvement in fidelity, consistency,
                        and motion over Gen-2, and a step towards building General World Models.</i></p>
                <span><b>Explore More: </b></span><a href="https://runwayml.com/blog/introducing-gen-3-alpha/"
                    class="explore-more">Blog</a>
            </div>
            <div class="item">
                <h3>Text to Sound Effects Model from ElevenLabs</h3>
                <p><i>It's simple: just describe the sound you have in mind and Text to Sound Effects Model will
                        generate a few samples to choose
                        from. You can then upscale the one you like the most, or continue generating until you find the
                        sound effect that works for you.</i></p>
                <span><b>Explore More: </b></span><a href="https://elevenlabs.io/sound-effects"
                    class="explore-more">Website</a>
            </div>
            <div class="item">
                <h3>Release of Stable Diffusion 3 Medium from StabilityAI</h3>
                <p><i>SD3 Medium is a 2 billion parameter AI model that generates high-quality, photorealistic images
                        with improved handling of hands, faces, and text. It excels in understanding complex prompts,
                        runs efficiently on consumer GPUs, and is suitable for fine-tuning on small datasets.</i></p>
                <span><b>Explore More: </b></span><a href="https://stability.ai/news/stable-diffusion-3-medium"
                    class="explore-more">Blog</a>
            </div>
            <div class="item">
                <h3>Release of long awaited Florence-2 Model from Microsoft</h3>
                <p><i>Florence-2 is a versatile vision foundation model that uses text prompts to perform various vision
                        and vision-language tasks. It employs a sequence-to-sequence architecture and leverages a
                        massive dataset to excel in multi-task learning, demonstrating strong performance in both
                        zero-shot and fine-tuned scenarios.</i></p>
                <span><b>Explore More: </b></span><a href="https://huggingface.co/microsoft/Florence-2-large"
                    class="explore-more">HuggingFace</a><span>|</span> <a href="https://arxiv.org/abs/2311.06242"
                    class="explore-more">Paper</a> <span>|</span> <a
                    href="https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb"
                    class="explore-more">Colab Notebook</a>
            </div>
        </section>

        <section class="hot-research">
            <h2>What's Hot in Research?</h2>
            <div class="item">
                <h3>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</h3>
                <p><i>4M-21 outperforms multimodal models like 4M and UnifiedIO by training on tens of diverse
                        modalities, expanding their capabilities to handle 3x more tasks without performance loss, and
                        enabling fine-grained, controllable generation.</i></p>
                <span><b>Explore More: </b></span><a href="https://4m.epfl.ch/" class="explore-more">Website</a>
                <span>|</span> <a href="https://arxiv.org/abs/2406.09406" class="explore-more">Paper</a>
            </div>
            <div class="item">
                <h3>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</h3>
                <p><i>Cambrian-1 is a new family of vision-centric multimodal language models. It focuses on improving
                        visual components through better encoders, a new connector design, high-quality instruction
                        data, refined tuning strategies, and comprehensive benchmarking.</i></p>
                <span><b>Explore More: </b></span><a href="https://cambrian-mllm.github.io/"
                    class="explore-more">Website</a>
                <span>|</span> <a href="https://arxiv.org/abs/2406.16860" class="explore-more">Paper</a>
            </div>
            <div class="item">
                <h3>GAMA:
                    A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities</h3>
                <p><i>GAMA, a General-purpose Large Audio-Language Model, integrates a language model with advanced
                        audio representations and fine-tuned for audio understanding and complex reasoning. Evaluations
                        show GAMA outperforms existing models in audio tasks by significant margins.</i></p>
                <span><b>Explore More: </b></span><a href="https://sreyan88.github.io/gamaaudio/"
                    class="explore-more">Website</a>
                <span>|</span> <a href="https://arxiv.org/abs/2406.11768" class="explore-more">Paper</a>
            </div>
            <div class="item">
                <h3>MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding</h3>
                <p><i>MuirBench is a comprehensive benchmark for evaluating multi-image understanding capabilities of
                        multimodal LLMs. It includes 12 diverse tasks across 10 categories of multi-image relations,
                        with 11,264 images and 2,600 questions.</i></p>
                <span><b>Explore More: </b></span><a href="https://muirbench.github.io/"
                    class="explore-more">Website</a>
                <span>|</span> <a href="https://arxiv.org/abs/2406.09411" class="explore-more">Paper</a>
            </div>
        </section>

        <section class="knowledge-arsenal">
            <h2>Boost Your Knowledge Arsenal</h2>
            <div class="item">
                <h3>Step-by-Step Diffusion: An Elementary Tutorial</h3>
                <p><i>A 101 tutorial on diffusion models, mostly used in text-image generation and flow matching for
                        machine
                        learning. It presents key concepts and algorithms while minimizing complex math.</i></p>
                <span><b>Resources: </b></span><a href="https://arxiv.org/pdf/2406.08929" class="learn-more">Paper</a>
                <div class="difficulty-level" data-difficulty="beginner">
                    <button class="difficulty-btn">Beginner</button>
                </div>
            </div>
            <div class="item">
                <h3>Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4</h3>
                <p><i>The talk consists of building large multimodal models like GPT-4, including discussions on data,
                        instruction tuning, architecture, parameter-efficient fine-tuning, and evaluation.</i></p>
                <span><b>Resources: </b></span><a href="https://www.youtube.com/watch?v=mkI7EPD1vp8"
                    class="learn-more">YouTube</a>
                <div class="difficulty-level" data-difficulty="beginner">
                    <button class="difficulty-btn">Beginner</button>
                </div>
            </div>
            <div class="item">
                <h3>UNet Diffusion Model in Pure CUDA</h3>
                <p><i>The repository consists of writing code for a UNet Diffusion Model from scratch in CUDA. This
                        might
                        be a fun exercise for those who want to understand how CUDA kernels are written for Diffusion
                        Models.</i></p>
                <span><b>Resources: </b></span><a href="https://github.com/clu0/unet.cu" class="learn-more">GitHub</a>
                <div class="difficulty-level" data-difficulty="advanced">
                    <button class="difficulty-btn">Advanced</button>
                </div>
            </div>
        </section>

        <section class="community-recap">
            <h2>Recap Multimodal Community (June 2024)</h2>
            <div class="item">
                <h3>Paper Reading Session - June 21st</h3>
                <p><i>The session was led by Surya Guthikonda, where he discussed the chapters on Introduction, Visual
                        Understanding, Visual Generation, and Unified Vision Models from the paper "Multimodal
                        Foundation
                        Models: From Specialists to General-Purpose Assistants."</i></p>
                <span><b>Resources: </b></span><a
                    href="https://docs.google.com/presentation/d/1uR0jl1gv4o_hSmUzJKnk--Vm8U__NE_rYmJ6q8OA270/edit?usp=sharing"
                    class="explore-more">Slides</a> <span>|</span> <a href="https://arxiv.org/abs/2309.10020"
                    class="explore-more">Paper</a>
                <div class="difficulty-level" data-difficulty="beginner">
                    <button class="difficulty-btn">Beginner</button>
                </div>
            </div>
            <div class="item">
                <h3>Paper Reading Session - June 28th</h3>
                <p><i>The session was led by Henry Vo, where he discussed the chapters on Large MultiModal Models:
                        Training with LLMs, Multimodal Agents: Chaining with Tools, and Conclusion and Research Trends
                        from the paper "Multimodal
                        Foundation
                        Models: From Specialists to General-Purpose Assistants."</i></p>
                <span><b>Resources: </b></span><a
                    href="https://drive.google.com/file/d/17lmF51-NmSqkeZUEPk1tOgphmmqxJPoS/view"
                    class="explore-more">Recording</a> <span>|</span> <a href="https://arxiv.org/abs/2309.10020"
                    class="explore-more">Paper</a>
                <div class="difficulty-level" data-difficulty="intermediate">
                    <button class="difficulty-btn">Intermediate</button>
                </div>
            </div>
        </section>

        <section class="disclaimer">
            <h2>Disclaimer</h2>
            <p>Newsletter highlights notable recent multimodal AI developments but is not exhaustive. We acknowledge
                that we might have missed some exceptional works.</p>
        </section>

    </main>

    <footer>
        <p>&copy; 2024 MultiModal AI @ Cohere For AI Community. All rights reserved.</p>
    </footer>

    <script src="../../src/scripts/newsletter.js"></script>
</body>

</html>