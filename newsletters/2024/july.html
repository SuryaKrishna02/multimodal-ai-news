<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MultiModal AI Newsletter - July 2024 Edition</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../src/styles/newsletter.css">
</head>

<body>
    <header>
        <nav>
            <a href="../../index.html">Home</a>
            <a href="https://sites.google.com/cohere.com/c4ai-community/community-programs/multimodal"
                target="_blank">Community</a>
            <a href="index.html#about-us">About Us</a>
        </nav>
    </header>

    <main>
        <section class="newsletter-header">
            <h1>Multimodal AI Newsletter</h1>
            <h2>July 2024 Edition</h2>
        </section>

        <section class="tech-pulse">
            <h2>The Tech Pulse</h2>
            <div class="item">
                <h3>Item 1 Name</h3>
                <p>Item Description</p>
                <div class="item-footer">
                    <a href="#" class="explore-more">Explore more: Links</a>
                </div>
            </div>
        </section>

        <section class="hot-research">
            <h2>What's Hot in Research?</h2>
            <div class="item">
                <h3>Item 1 Name</h3>
                <p>Item Description</p>
                <div class="item-footer">
                    <a href="#" class="explore-more">Explore more: Links</a>
                </div>
            </div>
        </section>

        <section class="knowledge-arsenal">
            <h2>Boost Your Knowledge Arsenal</h2>
            <div class="item">
                <h3>Step-by-Step Diffusion: An Elementary Tutorial</h3>
                <p>A 101 tutorial on diffusion models, mostly used in text-image generation and flow matching for
                    machine
                    learning. It presents key concepts and algorithms while minimizing complex math.</p>
                <span><b>Resources: </b></span><a href="https://arxiv.org/pdf/2406.08929" class="learn-more">Paper</a>
                <div class="difficulty-level" data-difficulty="beginner">
                    <span>Difficulty:</span>
                    <button class="difficulty-btn">Beginner</button>
                </div>
            </div>
            <div class="item">
                <h3>Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4</h3>
                <p>The talk consists of building large multimodal models like GPT-4, including discussions on data,
                    instruction tuning, architecture, parameter-efficient fine-tuning, and evaluation.</p>
                <span><b>Resources: </b></span><a href="https://www.youtube.com/watch?v=mkI7EPD1vp8"
                    class="learn-more">YouTube</a>
                <div class="difficulty-level" data-difficulty="beginner">
                    <span>Difficulty:</span>
                    <button class="difficulty-btn">Beginner</button>
                </div>
            </div>
            <div class="item">
                <h3>UNet Diffusion Model in Pure CUDA</h3>
                <p>The repository consists of writing code for a UNet Diffusion Model from scratch in CUDA. This might
                    be a fun exercise for those who want to understand how CUDA kernels are written for Diffusion
                    Models.</p>
                <span><b>Resources: </b></span><a href="https://github.com/clu0/unet.cu" class="learn-more">GitHub</a>
                <div class="difficulty-level" data-difficulty="advanced">
                    <span>Difficulty:</span>
                    <button class="difficulty-btn">Advanced</button>
                </div>
            </div>
        </section>

        <section class="community-recap">
            <h2>Recap Multimodal Community (June 2024)</h2>
            <div class="item">
                <h3>Paper Reading Session - June 21st</h3>
                <p><i>The session was led by Surya Guthikonda, where he discussed the chapters on Introduction, Visual
                        Understanding, Visual Generation, and Unified Vision Models from the paper "Multimodal
                        Foundation
                        Models: From Specialists to General-Purpose Assistants."</i></p>
                <span><b>Resources: </b></span><a
                    href="https://docs.google.com/presentation/d/1uR0jl1gv4o_hSmUzJKnk--Vm8U__NE_rYmJ6q8OA270/edit?usp=sharing"
                    class="explore-more">Slides</a> <span>|</span> <a href="https://arxiv.org/abs/2309.10020"
                    class="explore-more">Paper</a>
                <div class="difficulty-level" data-difficulty="beginner">
                    <span><b>Difficulty:</b></span>
                    <button class="difficulty-btn">Beginner</button>
                </div>
            </div>
            <div class="item">
                <h3>Paper Reading Session - June 28th</h3>
                <p><i>The session was led by Henry Vo, where he discussed the chapters on Large MultiModal Models:
                        Training with LLMs, Multimodal Agents: Chaining with Tools, and Conclusion and Research Trends
                        from the paper "Multimodal
                        Foundation
                        Models: From Specialists to General-Purpose Assistants."</i></p>
                <span><b>Resources: </b></span><a
                    href="https://drive.google.com/file/d/17lmF51-NmSqkeZUEPk1tOgphmmqxJPoS/view"
                    class="explore-more">Recording</a> <span>|</span> <a href="https://arxiv.org/abs/2309.10020"
                    class="explore-more">Paper</a>
                <div class="difficulty-level" data-difficulty="intermediate">
                    <span><b>Difficulty:</b></span>
                    <button class="difficulty-btn">Intermediate</button>
                </div>
            </div>
        </section>

        <section class="disclaimer">
            <h2>Disclaimer</h2>
            <p>Newsletter highlights notable recent multimodal AI developments but is not exhaustive. We acknowledge
                that we might have missed some exceptional works.</p>
        </section>

    </main>

    <footer>
        <p>&copy; 2024 MultiModal AI @ Cohere For AI Community. All rights reserved.</p>
    </footer>

    <script src="../../src/scripts/newsletter.js"></script>
</body>

</html>